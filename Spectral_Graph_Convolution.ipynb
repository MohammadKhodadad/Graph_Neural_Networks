{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obfjbiJpNQ5-"
      },
      "outputs": [],
      "source": [
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kVkBtorGFkv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import h5py\n",
        "import torch\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as sio\n",
        "import torch.nn.functional as F\n",
        "import plotly.express as px\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torch.utils.data import Dataset\n",
        "from torch_geometric.data import Data,HeteroData\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.datasets import MovieLens\n",
        "from torch_geometric.nn import RGCNConv\n",
        "from torch_geometric.utils import dropout_adj\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKvSOmOzG9M7"
      },
      "outputs": [],
      "source": [
        "\n",
        "def download():\n",
        "    BASE_DIR = './'\n",
        "    DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        os.mkdir(DATA_DIR)\n",
        "    if not os.path.exists(os.path.join(DATA_DIR, 'modelnet40_ply_hdf5_2048')):\n",
        "        www = 'https://shapenet.cs.stanford.edu/media/modelnet40_ply_hdf5_2048.zip'\n",
        "        zipfile = os.path.basename(www)\n",
        "        os.system('wget --no-check-certificate %s; unzip %s' % (www, zipfile))\n",
        "        os.system('mv %s %s' % (zipfile[:-4], DATA_DIR))\n",
        "        os.system('rm %s' % (zipfile))\n",
        "\n",
        "\n",
        "def load_data(partition):\n",
        "    download()\n",
        "    BASE_DIR = './'\n",
        "    DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
        "    all_data = []\n",
        "    all_label = []\n",
        "    for h5_name in glob.glob(os.path.join(DATA_DIR, 'modelnet40_ply_hdf5_2048', 'ply_data_%s*.h5'%partition)):\n",
        "        f = h5py.File(h5_name)\n",
        "        data = f['data'][:].astype('float32')\n",
        "        label = f['label'][:].astype('int64')\n",
        "        f.close()\n",
        "        all_data.append(data)\n",
        "        all_label.append(label)\n",
        "    all_data = np.concatenate(all_data, axis=0)\n",
        "    all_label = np.concatenate(all_label, axis=0)\n",
        "    return all_data, all_label\n",
        "\n",
        "\n",
        "def load_scanobjectnn_data(partition):\n",
        "    BASE_DIR = './'\n",
        "    DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
        "    all_data = []\n",
        "    all_label = []\n",
        "\n",
        "    h5_name = BASE_DIR + '/data/' + partition + '_objectdataset_augmentedrot_scale75.h5'\n",
        "    f = h5py.File(h5_name)\n",
        "    data = f['data'][:].astype('float32')\n",
        "    label = f['label'][:].astype('int64')\n",
        "    f.close()\n",
        "    all_data.append(data)\n",
        "    all_label.append(label)\n",
        "    all_data = np.concatenate(all_data, axis=0)\n",
        "    all_label = np.concatenate(all_label, axis=0)\n",
        "    return all_data, all_label\n",
        "\n",
        "\n",
        "def translate_pointcloud(pointcloud):\n",
        "    xyz1 = np.random.uniform(low=2./3., high=3./2., size=[3])\n",
        "    xyz2 = np.random.uniform(low=-0.2, high=0.2, size=[3])\n",
        "       \n",
        "    translated_pointcloud = np.add(np.multiply(pointcloud, xyz1), xyz2).astype('float32')\n",
        "    return translated_pointcloud\n",
        "\n",
        "\n",
        "def jitter_pointcloud(pointcloud, sigma=0.01, clip=0.02):\n",
        "    N, C = pointcloud.shape\n",
        "    pointcloud += np.clip(sigma * np.random.randn(N, C), -1*clip, clip)\n",
        "    return pointcloud\n",
        "\n",
        "\n",
        "class ModelNet40(Dataset):\n",
        "    def __init__(self, num_points, partition='train'):\n",
        "        self.data, self.label = load_data(partition)\n",
        "        self.num_points = num_points\n",
        "        self.partition = partition        \n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        pointcloud = self.data[item][:self.num_points]\n",
        "        label = self.label[item]\n",
        "        if self.partition == 'train':\n",
        "            pointcloud = translate_pointcloud(pointcloud)\n",
        "            np.random.shuffle(pointcloud)\n",
        "        return torch.tensor(pointcloud), torch.tensor(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "\n",
        "class ScanObjectNN(Dataset):\n",
        "    def __init__(self, num_points, partition='training'):\n",
        "        self.data, self.label = load_scanobjectnn_data(partition)\n",
        "        self.num_points = num_points\n",
        "        self.partition = partition        \n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        pointcloud = self.data[item][:self.num_points]\n",
        "        label = self.label[item]\n",
        "        if self.partition == 'training':\n",
        "            pointcloud = translate_pointcloud(pointcloud)\n",
        "            np.random.shuffle(pointcloud)\n",
        "        return pointcloud, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train=ModelNet40(partition='train', num_points=128)\n",
        "\n",
        "test=ModelNet40(partition='test', num_points=128)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HllTzKNoHBza",
        "outputId": "56df27a2-243e-477b-929e-c43a6c405d69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ],
      "source": [
        "train_loader=DataLoader(train,batch_size=64)\n",
        "test_loader=DataLoader(test,batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u65gKRrDM9no"
      },
      "outputs": [],
      "source": [
        "class EDM(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "    ):\n",
        "        super(EDM, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(3,32)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(32)\n",
        "        self.fc2 = torch.nn.Linear(64,64)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
        "        self.fc3 = torch.nn.Linear(192,128)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(128)\n",
        "        self.output=torch.nn.Linear(128,40)\n",
        "\n",
        "\n",
        "    def extract_distance_matrix(self,inp):\n",
        "        x1=torch.unsqueeze(inp,2)\n",
        "        x2=torch.unsqueeze(inp,1)\n",
        "        dist=torch.sqrt(torch.sum(((x1-x2)**2),dim=-1))\n",
        "        max_=torch.max(dist,keepdim=True,dim=-1)[0]\n",
        "        max_=torch.max(max_,keepdim=True,dim=-1)[0]\n",
        "        weight=max_-dist\n",
        "        # ones=torch.diag_embed(torch.ones(inp.size(0),inp.size(1)))\n",
        "        # weight=weight*(1-ones)\n",
        "\n",
        "        return weight\n",
        "\n",
        "\n",
        "\n",
        "    def normalize_distance_matrix(self,inp):\n",
        "        D = torch.sum(inp , dim = 2)\n",
        "\n",
        "        D_sqrt = torch.div(1.0 , torch.sqrt(D))\n",
        "        D_sqrt = torch.diag_embed(D_sqrt)\n",
        "\n",
        "        I = torch.ones_like(D , dtype = torch.float32)\n",
        "        I = torch.diag_embed( I )\n",
        "        normalized_weight = I - torch.bmm(D_sqrt , torch.bmm(inp , D_sqrt))\n",
        "        return normalized_weight\n",
        "\n",
        "    def extract_simplified_SVD(self,inp):\n",
        "        # U, S, Vh = torch.linalg.svd(inp)\n",
        "        S,U=torch.linalg.eig(inp)\n",
        "        S=S.float()\n",
        "        U=U.float()\n",
        "        Vh=torch.permute(U,(0,2,1))\n",
        "        # S[:,32:]=0\n",
        "        return U[:,:,:32] , torch.diag_embed(S[:,:32]) , Vh[:,:32,:]\n",
        "\n",
        "\n",
        "    def forward(self,inp):\n",
        "        weight_matrix=self.extract_distance_matrix(inp)\n",
        "        nw=self.normalize_distance_matrix(weight_matrix)\n",
        "        # nw_s=self.extract_simplified_SVD(nw)\n",
        "        U,S,Vh=self.extract_simplified_SVD(nw)\n",
        "        \n",
        "\n",
        "        x=self.fc1(inp)\n",
        "        x=F.relu(torch.transpose(self.bn1(torch.transpose(x,1,2)),1,2))\n",
        "        x_gcn=F.relu(torch.bmm(U,torch.bmm(S,torch.bmm(Vh,x))))\n",
        "        x1=torch.cat([x,x_gcn],dim=-1)\n",
        "        \n",
        "\n",
        "        x=self.fc2(x1)\n",
        "        x=F.relu(torch.transpose(self.bn2(torch.transpose(x,1,2)),1,2))\n",
        "        x_gcn=F.relu(torch.bmm(U,torch.bmm(S,torch.bmm(Vh,x))))\n",
        "        x2=torch.cat([x,x_gcn],dim=-1)\n",
        "        \n",
        "\n",
        "        x=torch.cat([x1,x2],dim=-1)\n",
        "        x=self.fc3(x)#n,192,128\n",
        "        x=F.relu(torch.transpose(self.bn3(torch.transpose(x,1,2)),1,2))\n",
        "\n",
        "        x=F.max_pool1d(torch.permute(x,(0,2,1)),kernel_size=128).view(-1,128)\n",
        "        x=self.output(F.dropout(x,0.3)) ## REMOVE SOFTMAX IT EXISTS IN CROSSENTROPY\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86wLPRY_NyH4"
      },
      "outputs": [],
      "source": [
        "def loss_fn(output,gt):\n",
        "      gt=gt.view(-1)\n",
        "      loss= F.cross_entropy(output,gt)\n",
        "      max_=torch.argmax(output,dim=-1).view(-1)\n",
        "      sum_=(max_==gt).sum()\n",
        "      return loss,sum_,gt.size(0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWlCtQ5Sv17S"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = EDM().to(device)\n",
        "def train():\n",
        "    \n",
        "    \n",
        "    optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
        "    max_val_acc=0\n",
        "    for epoch in range(500):\n",
        "      \n",
        "      model.train()\n",
        "      train_loss=0\n",
        "      train_corrects=0\n",
        "      train_records=0\n",
        "\n",
        "      for data,label in train_loader:\n",
        "        data = data.to(device)\n",
        "        label=label.to(device)\n",
        "        out = model(data)\n",
        "        loss,sum_,num_= loss_fn(out,label,)\n",
        "        train_loss+=loss\n",
        "        train_corrects+=sum_\n",
        "        train_records+=num_\n",
        "      optimizer.zero_grad()\n",
        "      train_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "      test_loss=0\n",
        "      test_corrects=0\n",
        "      test_records=0\n",
        "      model.eval()\n",
        "      for data,label in test_loader:\n",
        "        data = data.to(device)\n",
        "        label=label.to(device)\n",
        "        out = model(\n",
        "            data\n",
        "        )\n",
        "        loss,sum_,num_=loss_fn(out,label,)\n",
        "        test_loss+=loss\n",
        "        test_corrects+=sum_\n",
        "        test_records+=num_\n",
        "      if test_corrects/test_records >max_val_acc:\n",
        "        max_val_acc=test_corrects/test_records\n",
        "      print(f'Epoch: {epoch}, Loss: {train_loss.item()},\"Acc\":{train_corrects/train_records},  val_Loss: {test_loss.item()}, val_Acc:{test_corrects/test_records}, max_val_Acc: {max_val_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tG8EQZQJwpXq"
      },
      "outputs": [],
      "source": [
        "train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}