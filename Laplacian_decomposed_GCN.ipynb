{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8K-M37M0BFI1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as sio\n",
        "import tensorflow as tf\n",
        "import plotly.express as px\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import h5py\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def download():\n",
        "    BASE_DIR = './'\n",
        "    DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        os.mkdir(DATA_DIR)\n",
        "    if not os.path.exists(os.path.join(DATA_DIR, 'modelnet40_ply_hdf5_2048')):\n",
        "        www = 'https://shapenet.cs.stanford.edu/media/modelnet40_ply_hdf5_2048.zip'\n",
        "        zipfile = os.path.basename(www)\n",
        "        os.system('wget --no-check-certificate %s; unzip %s' % (www, zipfile))\n",
        "        os.system('mv %s %s' % (zipfile[:-4], DATA_DIR))\n",
        "        os.system('rm %s' % (zipfile))\n",
        "\n",
        "\n",
        "def load_data(partition):\n",
        "    download()\n",
        "    BASE_DIR = './'\n",
        "    DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
        "    all_data = []\n",
        "    all_label = []\n",
        "    for h5_name in glob.glob(os.path.join(DATA_DIR, 'modelnet40_ply_hdf5_2048', 'ply_data_%s*.h5'%partition)):\n",
        "        f = h5py.File(h5_name)\n",
        "        data = f['data'][:].astype('float32')\n",
        "        label = f['label'][:].astype('int64')\n",
        "        f.close()\n",
        "        all_data.append(data)\n",
        "        all_label.append(label)\n",
        "    all_data = np.concatenate(all_data, axis=0)\n",
        "    all_label = np.concatenate(all_label, axis=0)\n",
        "    return all_data, all_label\n",
        "\n",
        "\n",
        "def load_scanobjectnn_data(partition):\n",
        "    BASE_DIR = './'\n",
        "    DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
        "    all_data = []\n",
        "    all_label = []\n",
        "\n",
        "    h5_name = BASE_DIR + '/data/' + partition + '_objectdataset_augmentedrot_scale75.h5'\n",
        "    f = h5py.File(h5_name)\n",
        "    data = f['data'][:].astype('float32')\n",
        "    label = f['label'][:].astype('int64')\n",
        "    f.close()\n",
        "    all_data.append(data)\n",
        "    all_label.append(label)\n",
        "    all_data = np.concatenate(all_data, axis=0)\n",
        "    all_label = np.concatenate(all_label, axis=0)\n",
        "    return all_data, all_label\n",
        "\n",
        "\n",
        "def translate_pointcloud(pointcloud):\n",
        "    xyz1 = np.random.uniform(low=2./3., high=3./2., size=[3])\n",
        "    xyz2 = np.random.uniform(low=-0.2, high=0.2, size=[3])\n",
        "       \n",
        "    translated_pointcloud = np.add(np.multiply(pointcloud, xyz1), xyz2).astype('float32')\n",
        "    return translated_pointcloud\n",
        "\n",
        "\n",
        "def jitter_pointcloud(pointcloud, sigma=0.01, clip=0.02):\n",
        "    N, C = pointcloud.shape\n",
        "    pointcloud += np.clip(sigma * np.random.randn(N, C), -1*clip, clip)\n",
        "    return pointcloud\n",
        "\n",
        "\n",
        "class ModelNet40(Dataset):\n",
        "    def __init__(self, num_points, partition='train'):\n",
        "        self.data, self.label = load_data(partition)\n",
        "        self.num_points = num_points\n",
        "        self.partition = partition        \n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        pointcloud = self.data[item][:self.num_points]\n",
        "        label = self.label[item]\n",
        "        if self.partition == 'train':\n",
        "            pointcloud = translate_pointcloud(pointcloud)\n",
        "            np.random.shuffle(pointcloud)\n",
        "        return pointcloud, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "\n",
        "class ScanObjectNN(Dataset):\n",
        "    def __init__(self, num_points, partition='training'):\n",
        "        self.data, self.label = load_scanobjectnn_data(partition)\n",
        "        self.num_points = num_points\n",
        "        self.partition = partition        \n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        pointcloud = self.data[item][:self.num_points]\n",
        "        label = self.label[item]\n",
        "        if self.partition == 'training':\n",
        "            pointcloud = translate_pointcloud(pointcloud)\n",
        "            np.random.shuffle(pointcloud)\n",
        "        return pointcloud, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train=ModelNet40(partition='train', num_points=1024)\n",
        "\n",
        "test=ModelNet40(partition='test', num_points=1024)\n",
        "\n",
        "X_train=train.data[:,:128,:]\n",
        "X_test=test.data[:,:128,:]\n",
        "y_train=train.label\n",
        "y_test=test.label\n",
        "\n",
        "y_train=np.eye(40)[y_train[:,0]]\n",
        "y_test=np.eye(40)[y_test[:,0]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#l=X_train.shape[0]\n",
        "#ch=np.random.choice(l,int(l/1))\n",
        "#X_train=X_train[ch]\n",
        "#y_train=y_train[ch]"
      ],
      "metadata": {
        "id": "FLzhTz2rBGq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DECOMPOSED LAPLACIAN\n",
        "print(\"LOADING MODEL\")\n",
        "def conv_bn(x, filters,):\n",
        "    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\",kernel_regularizer=tf.keras.regularizers.l2( l=0.000005))(x)\n",
        "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "    return layers.Activation(\"relu\")(x)\n",
        "\n",
        "def conv2_bn(x, filters,):\n",
        "    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\",kernel_regularizer=tf.keras.regularizers.l2( l=0.000005))(x)\n",
        "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "    return x\n",
        "\n",
        "def dense_bn(x, filters):\n",
        "    x = layers.Dense(filters,kernel_regularizer=tf.keras.regularizers.l2( l=0.000005))(x)\n",
        "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "    return layers.Activation(\"relu\")(x)\n",
        "\n",
        "\n",
        "\n",
        "def matrix_block(inp,matrix,output_shape=32):\n",
        "  x1=conv_bn(inp,output_shape)\n",
        "  x2=K.batch_dot(matrix,x1,[2,1])             \n",
        "  x2 = tf.keras.layers.BatchNormalization(momentum=0.0)(x2)\n",
        "  return tf.keras.layers.concatenate([x1,x2],axis=-1)\n",
        "\n",
        "class simplify_e(tf.keras.layers.Layer): \n",
        "   def __init__(self,n=32,**kwargs): \n",
        "      super(simplify_e, self).__init__(**kwargs)\n",
        "      self.n=n\n",
        "   def call(self, e):\n",
        "      bs=K.shape(e)[0]\n",
        "      first_shape=e.shape[1]\n",
        "      mask1=tf.ones((bs,self.n))\n",
        "      mask2=tf.zeros((bs,128-self.n))\n",
        "      mask=tf.concat([mask1,mask2],axis=1)\n",
        "      out=e*mask\n",
        "      return out\n",
        "\n",
        "inp_spatial=tf.keras.layers.Input((X_train.shape[1],3))\n",
        "\n",
        "x=conv_bn(inp_spatial,8)\n",
        "\n",
        "out=tf.sqrt(tf.reduce_sum((tf.expand_dims(x, 2)-tf.expand_dims(x, 1))**2,axis=3))\n",
        "\n",
        "mask_out=tf.linalg.diag(np.ones((1,128)).astype(np.float32))*1\n",
        "out=out*(1-mask_out)\n",
        "max_=tf.math.reduce_max(out,axis=-1,keepdims=True)\n",
        "max_=tf.math.reduce_max(max_,axis=-2,keepdims=True)\n",
        "out=max_-out\n",
        "D = tf.reduce_sum(out , axis = 2)\n",
        "D_sqrt = tf.divide(1.0 , tf.sqrt(D))\n",
        "D_sqrt = tf.linalg.diag(D_sqrt)\n",
        "\n",
        "I = tf.ones_like(D , dtype = tf.float32)\n",
        "I = tf.linalg.diag( I )\n",
        "out = I - K.batch_dot(D_sqrt , K.batch_dot(out , D_sqrt,[2,1]),[2,1])\n",
        "\n",
        "\n",
        "e,v=tf.linalg.eigh(\n",
        "    out, name=None\n",
        ")\n",
        "e2=simplify_e()(e)\n",
        "\n",
        "e2=tf.linalg.diag(e2)\n",
        "left=K.batch_dot(v,e2,[2,1])\n",
        "matrix_simp=K.batch_dot(left,tf.transpose(v,[0,2,1]),[2,1])\n",
        "\n",
        "\n",
        "x1=matrix_block(inp_spatial,matrix_simp,32)\n",
        "x2=matrix_block(x1,matrix_simp,128)\n",
        "x_all=conv_bn(x2,256)\n",
        "flatten=tf.keras.layers.GlobalMaxPool1D()(x_all)\n",
        "dp=tf.keras.layers.Dropout(0.5)(flatten)\n",
        "\n",
        "\n",
        "\n",
        "outputs = layers.Dense(y_train.shape[-1], activation=\"softmax\")(dp)\n",
        "\n",
        "model = keras.Model(inputs=inp_spatial, outputs=outputs, name=\"pointnet\")\n",
        "model.summary()\n",
        "BATCH_SIZE = 64\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=[\"categorical_accuracy\"],\n",
        ")\n",
        "checkpoint_filepath = 'weights/bl1024v1_1_2.ckpt'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_categorical_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 20:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.003)\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "class_weight = dict(zip(np.unique(y_train.argmax(axis=-1)),compute_class_weight(class_weight = \"balanced\", classes= np.unique(y_train.argmax(axis=-1)), y= y_train.argmax(axis=-1))))\n",
        "\n",
        "model.fit(X_train,y_train, epochs=2000, batch_size=BATCH_SIZE, validation_data=(X_test,y_test),callbacks=[model_checkpoint_callback,lr_callback],class_weight = class_weight)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k8lGeaAVBIjc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}